{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "whole_dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_datasets = whole_dataset['train'].train_test_split(test_size=0.0005, seed=42)\n",
    "\n",
    "# Access the training and testing sets\n",
    "train_dataset = split_datasets['train']\n",
    "test_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "model_path = 'Mistral-7B-v0.1-fine-tuned'\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=finetuned_model,\n",
    "    tokenizer=finetuned_tokenizer,\n",
    "    device=0,\n",
    "    # top_k=50,  # Set top_k to your desired value\n",
    "    # num_beams=5,  # Set beam_size to your desired value\n",
    "    # temperature=1  # Set temperature to your desired value\n",
    ")\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "num_examples = len(test_dataset)\n",
    "print(num_examples)\n",
    "total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "generated_output = []\n",
    "\n",
    "for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "    batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "    batch = test_dataset.select(batch_indices)\n",
    "    prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "    # print(prompts)\n",
    "    # Generate text for the batch\n",
    "    results = pipe(prompts, max_new_tokens=64)\n",
    "    \n",
    "    for result in results:\n",
    "        generated_text = result[0]['generated_text']\n",
    "        generated_output.append(generated_text)\n",
    "\n",
    "        # Uncomment the following lines if you want to print the prompts and generated text\n",
    "        prompt = prompts[results.index(result)]\n",
    "        # print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated Text: {generated_text}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_texts = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "generated_texts = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output]\n",
    "# generated_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Reference answer:\n",
      "For someone with arthritis, the best type of exercise would be low-impact activities like yoga, swimming, or walking. These exercises provide the benefits of exercise without exacerbating the symptoms of arthritis.\n",
      "\n",
      "Generated answer:\n",
      "Low-impact exercises such as swimming, walking, and yoga are recommended for people with arthritis. These exercises help to strengthen the muscles and joints without putting too much strain on the joints. Additionally, stretching and range of motion exercises can help to improve flexibility and\n",
      "\n",
      "1\n",
      "Reference answer:\n",
      "The atomic mass for lithium is 6.941 u (units). This is determined from the number of protons and neutrons in the nucleus of a lithium atom, which is 3 protons and 4 neutrons. Each proton and neutron has a mass of 1.007 u, resulting in a total mass of 6.941 u.\n",
      "\n",
      "Generated answer:\n",
      "The atomic mass for lithium is 6.941. \n",
      "\n",
      "The atomic mass for lithium is 6.941. This is the mass of one atom of lithium, which is made up of three protons and three neutrons.\n",
      "\n",
      "2\n",
      "Reference answer:\n",
      "The ASCII characters for the binary code is: wast.\n",
      "\n",
      "Generated answer:\n",
      "Hello World! \n",
      "\n",
      "01110101 01100001 0111001\n",
      "\n",
      "3\n",
      "Reference answer:\n",
      "She was trembling with fear, her heart racing wildly and her breath coming in short, panicked gasps. She felt her palms begin to sweat and her stomach tying itself into knots.  She was scared.\n",
      "\n",
      "Generated answer:\n",
      "She was trembling with fear. Her heart was pounding and her breath was shallow. She felt like she was in a nightmare. She was terrified. She was frozen with fear. She was paralyzed with fear. She was petrified with fear. She was petrified\n",
      "\n",
      "4\n",
      "Reference answer:\n",
      "1. Offer a free trial for a limited time.\n",
      "2. Give away several skateboards in a raffle or sweepstakes.\n",
      "3. Host a promotional event featuring the skateboard doing stunts in various locations.\n",
      "\n",
      "Generated answer:\n",
      "1. Create a video demonstrating the product's features and post it on social media.\n",
      "2. Offer a discount for customers who refer a friend.\n",
      "3. Host a giveaway for customers who purchase the product. \n",
      "4. Partner with local skate shops to\n",
      "\n",
      "5\n",
      "Reference answer:\n",
      "Pick up the book from the shelf.\n",
      "\n",
      "Generated answer:\n",
      "Pick up the book from the shelf. \n",
      "Please is not necessary in imperative mood. \n",
      "Pick up the book from the shelf, please. \n",
      "This is also acceptable. \n",
      "Please pick up the book from the shelf. \n",
      "This is also acceptable.\n",
      "\n",
      "6\n",
      "Reference answer:\n",
      "The four friends decided to take a picnic vacation to the countryside. They packed their sandwiches and snacks, filled thermoses with hot tea, and wore their warmest coats. They spent their days exploring the incredible landscapes, and in the evenings they sat around a campfire singing and enjoying each other's company. They spent their days immersed in nature, getting to know the hidden gems of the place. When they returned home, they were filled with beautiful memories that would last a lifetime.\n",
      "\n",
      "Generated answer:\n",
      "The four people are travelling together to a picnic vacation, where they will be able to enjoy the outdoors and take in the beauty of nature. They will be able to explore the area, go swimming, and have a picnic lunch. They will also be able to take part in\n",
      "\n",
      "7\n",
      "Reference answer:\n",
      "Big Data is the massive volume of data that is generated from companies’ interactions with customers, both online and offline. This data includes customer data such as age, gender, location, behaviors, preferences, and transactions. This data enables companies to gain insights into customer behavior and to develop data-driven marketing strategies. For customers, Big Data provides personalized and tailored experiences, as companies can use the data they have collected to offer more targeted content and customized products or services.\n",
      "\n",
      "Generated answer:\n",
      "Big Data is the term used to describe the large volumes of data that are generated by businesses and customers. It is used to gain insights into customer behavior, improve customer experience, and make better business decisions. Companies can use Big Data to better understand their customers, identify trends, and make more\n",
      "\n",
      "8\n",
      "Reference answer:\n",
      "The phrase \"wild goose chase\" is typically used to describe a futile and time-consuming endeavor. It can also connote a sense of futility, as trying to find something that one might never find is generally not seen as a worthwhile use of time and energy. Additionally, it implies a sense of confusion, as the phrase itself refers to a scavenger hunt with a disappearing prize.\n",
      "\n",
      "Generated answer:\n",
      "The phrase \"wild goose chase\" has a negative connotation, as it implies a futile and pointless pursuit. It suggests that the person is wasting their time and energy on something that will not lead to any meaningful outcome. It also implies that the person is being led\n",
      "\n",
      "9\n",
      "Reference answer:\n",
      "1. Journal of Online Learning Research \n",
      "2. Harvard Business Review \n",
      "3. Educational Technology & Society\n",
      "\n",
      "Generated answer:\n",
      "1. Online Learning Research Center\n",
      "2. Educational Technology Research and Development Journal\n",
      "3. National Center for Education Statistics\n",
      "4. Educause\n",
      "5. Association for Educational Communications and Technology\n",
      "6. International Society for Technology in Education\n",
      "7. Sloan Consortium\n",
      "\n",
      "\n",
      "10\n",
      "Reference answer:\n",
      "India, China, Japan, US\n",
      "\n",
      "Generated answer:\n",
      "India, China, Japan, US. \n",
      "\n",
      "The GDP per capita of India is the lowest, followed by China, Japan, and the US. This is because India has a large population and a relatively low GDP, while the US has a smaller population and a\n",
      "\n",
      "11\n",
      "Reference answer:\n",
      "The boat in the photo is a sailboat.\n",
      "\n",
      "Generated answer:\n",
      "The boat in the photo is a sailboat. It has a single mast and a triangular sail. It is typically used for recreational activities such as sailing and fishing. It is also known as a dinghy or a skiff. It is usually powered by wind and\n",
      "\n",
      "12\n",
      "Reference answer:\n",
      "Humor is one of the most important ingredients in life.\n",
      "\n",
      "Generated answer:\n",
      "Humor is one of the most important ingredients of life. \n",
      "(Note: The period is added at the end of the sentence.) \n",
      "(Note: The comma is added after the word \"life\".) \n",
      "(Note: The word \"of\" is capital\n",
      "\n",
      "13\n",
      "Reference answer:\n",
      "The length of the hypotenuse in the triangle is 5.  To calculate this, we use the Pythagorean theorem, which states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. In this case, the other two sides have lengths of 3 and 4, so the hypotenuse must have a length of 5.\n",
      "\n",
      "Generated answer:\n",
      "The length of the hypotenuse is 13.5. This can be calculated by using the Pythagorean Theorem, which states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. In this case\n",
      "\n",
      "14\n",
      "Reference answer:\n",
      "A symphony orchestra typically has anywhere from 60 to 100 different instruments in its ensemble.\n",
      "\n",
      "Generated answer:\n",
      "A symphony orchestra typically has between 60 and 100 instruments. This includes strings, woodwinds, brass, and percussion. The exact number of instruments can vary depending on the size of the orchestra and the type of music being performed. \n",
      "\n",
      "\n",
      "\n",
      "15\n",
      "Reference answer:\n",
      "GPT is a deep learning-based natural language processing architecture designed to generate highly realistic and human-like text and speech.\n",
      "\n",
      "Generated answer:\n",
      "GPT is a natural language processing architecture that uses deep learning to generate human-like text. It is capable of understanding and generating natural language, and can be used for a variety of tasks such as text summarization, question answering, and text generation. GPT is also capable of learning\n",
      "\n",
      "16\n",
      "Reference answer:\n",
      "Paris\n",
      "\n",
      "Generated answer:\n",
      "Paris. \n",
      "\n",
      "The capital of France is Paris. It is located in the north of the country and is the largest city in France. It is also one of the most visited cities in the world, known for its iconic landmarks such as the Eiffel Tower,\n",
      "\n",
      "17\n",
      "Reference answer:\n",
      "Here are five questions to better understand a customer's needs:\n",
      "\n",
      "1. What are your main objectives for this project? \n",
      "2. What have you tried before that didn't work?\n",
      "3. What is the main goal you're hoping to achieve? \n",
      "4. What resources do you currently have to achieve your goals? \n",
      "5. What portion of the work do you plan to outsource, and what will be done in-house?\n",
      "\n",
      "Generated answer:\n",
      "1. What are your current pain points?\n",
      "2. What are your goals for the future?\n",
      "3. What features are most important to you?\n",
      "4. What do you like about our product?\n",
      "5. What do you dislike about our product?\n",
      "6. What\n",
      "\n",
      "18\n",
      "Reference answer:\n",
      "You can cook pork chops in a variety of ways. For instance, you can bake, fry, grill, or even sauté them. You can also season them with various herbs and spices to give them a unique flavor. Additionally, pork chops can be marinated to make them moist and tender.\n",
      "\n",
      "Generated answer:\n",
      "- Grilled pork chops with a garlic and herb marinade\n",
      "- Pork chops with a honey and mustard glaze\n",
      "- Pork chops with a sweet and sour sauce\n",
      "- Pork chops with a barbecue sauce\n",
      "- Pork\n",
      "\n",
      "19\n",
      "Reference answer:\n",
      "User: Hi, I'm wondering when can I expect my order to arrive.\n",
      "Bot: We offer free standard shipping on all orders and the expected delivery date should have been provided to you with the shipment confirmation email. If you need to double check, please provide me with the order details and I'll check the status of your order.\n",
      "\n",
      "Generated answer:\n",
      "Customer: Hi, I'm wondering if you can help me with my shipping order.\n",
      "\n",
      "Customer Service Representative: Sure, what can I help you with?\n",
      "\n",
      "Customer: I placed an order a few days ago and I haven't received it yet.\n",
      "\n",
      "Customer\n",
      "\n",
      "20\n",
      "Reference answer:\n",
      "The arithmetic mean is the sum of all the values in a set of data, divided by the number of values in the set. For example, if you have six values of 5, 2, 8, 4, 9, and 7, the arithmetic mean would be (5 + 2 + 8 + 4 + 9 + 7) / 6 = 6. \n",
      "\n",
      "To calculate the arithmetic mean for a set of data, you simply add up all the values and divide by the number of values in the set.\n",
      "\n",
      "Generated answer:\n",
      "Arithmetic mean is the average of a set of numbers. To calculate it, add all the numbers and divide by the number of numbers. For example, if you have the numbers 2, 4, 6, and 8, the arithmetic mean is (2\n",
      "\n",
      "21\n",
      "Reference answer:\n",
      "One method for detecting spam emails is to use a machine learning model that is trained on examples of known spam emails. The model can then analyze new emails for patterns that are indicative of spam emails, such as the presence of certain keywords. Additionally, filters can be used to block emails from suspicious senders and detect emails with suspicious content.\n",
      "\n",
      "Generated answer:\n",
      "One method for detecting spam emails is to use a combination of keyword filters, sender reputation checks, and content analysis. Keyword filters can be used to identify words or phrases that are commonly used in spam emails. Sender reputation checks can be used to identify emails from known sp\n",
      "\n",
      "22\n",
      "Reference answer:\n",
      "The program should take two integers as input and return their sum as output. It should be written in a programming language such as Python, Java, or Javascript.\n",
      "\n",
      "// Sample code:\n",
      "\n",
      "// Start of program\n",
      "int a;\n",
      "int b;\n",
      "int sum;\n",
      "\n",
      "a = input(); // read first number from the user\n",
      "b = input(); // read second number from the user\n",
      "sum = a + b; // calculate sum\n",
      "print(sum); // print the output\n",
      "// End of program\n",
      "\n",
      "Generated answer:\n",
      "def add_numbers(num1, num2):\n",
      "    return num1 + num2\n",
      "\n",
      "print(add_numbers(2, 3)) # Output: 5\n",
      "print(add_numbers(10, 15)) # Output: \n",
      "\n",
      "23\n",
      "Reference answer:\n",
      "To reduce the number of distractions when studying, it is best to have a dedicated workspace that is free from clutter and other distractions such as noise and technology. It is also important to set a schedule and stick to it, as well as take regular breaks. Additionally, it is beneficial to practice meditation or deep breathing to stay focused and clear the mind.\n",
      "\n",
      "Generated answer:\n",
      "One way to reduce distractions when studying is to create a dedicated study space and stick to a schedule. Additionally, it can be helpful to turn off notifications on your phone and other devices, and to limit the amount of time you spend on social media. Finally, it can be\n",
      "\n",
      "24\n",
      "Reference answer:\n",
      "Algorithm:\n",
      "1. Initialize Sum as 0.\n",
      "2. Set a variable i to 0.\n",
      "3. While i is less than or equal to 100, do the following: \n",
      "  a. If i is divisible by 5 or 3, add the value of i to Sum.\n",
      "  b. Increment i by 1.\n",
      "4. Return the value of Sum.\n",
      "\n",
      "Generated answer:\n",
      "Algorithm:\n",
      "1. Initialize a variable sum to 0.\n",
      "2. For each number from 1 to 100, check if it is a multiple of 3 or 5.\n",
      "3. If it is a multiple of 3, add it to sum\n",
      "\n",
      "25\n",
      "Reference answer:\n",
      "Government\n",
      "\n",
      "Generated answer:\n",
      "Government. \n",
      "(Note: Proper nouns are capitalized.) \n",
      "(Note: The word \"government\" is a proper noun because it refers to a specific entity.) \n",
      "(Note: The word \"regulations\" is not a proper\n",
      "\n",
      "26\n",
      "Reference answer:\n",
      "• What led you to pursue a career in software development?\n",
      "• Describe your experience with developing software applications.\n",
      "• What challenges have you faced in software development and how did you overcome them?\n",
      "• What techniques do you use to debug code?\n",
      "• How do you keep yourself up-to-date with the latest software development technologies and trends?\n",
      "• What type of software development project are you most proud of and why?\n",
      "• How do you ensure code quality and performance?\n",
      "• Describe your experience with Agile/Scrum methodology.\n",
      "• Talk about a time when you had to take initiative to solve a problem during a software development project.\n",
      "• Describe an instance when you had to collaborate with a team member to develop a complex software feature.\n",
      "• How do you handle difficult stakeholders?\n",
      "• Describe a difficult decision you made while working on a software development project.\n",
      "• Talk about a time when you had to adjust to a change in requirements and how you handled it.\n",
      "\n",
      "Generated answer:\n",
      "1. What experience do you have with software development?\n",
      "2. What programming languages do you have experience with?\n",
      "3. What software development tools have you used?\n",
      "4. What is your experience with agile development methodologies?\n",
      "5. What is your experience with software testing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def human_evaluation_print(dataset, generated_responses):\n",
    "\n",
    "    # Make sure you have the correct number of responses\n",
    "    assert len(dataset) == len(generated_responses), \"The number of generated responses must match the number of dataset entries\"\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        reference_answer = dataset[i]['output']\n",
    "        generated_answer = generated_responses[i]\n",
    "        print(i)\n",
    "        print(f\"Reference answer:\\n{reference_answer}\\n\\nGenerated answer:\\n{generated_answer}\\n\")\n",
    "\n",
    "human_evaluation_print(test_dataset, generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scores for Each Sample:**\n",
    "\n",
    "### Sample 1 (Atomic Mass of Lithium):\n",
    "- Grammatical Correctness: 0.8\n",
    "- Coherence: 0.8\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 0.87**\n",
    "\n",
    "### Sample 2:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 3:\n",
    "- Grammatical Correctness: N/A\n",
    "- Coherence: N/A\n",
    "- Correctness of Answer: 0\n",
    "- **Average: 0**\n",
    "\n",
    "### Sample 4:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 5:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 6:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 0.8\n",
    "- Correctness of Answer: 0.8\n",
    "- **Average: 0.87**\n",
    "\n",
    "### Sample 7:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 0.8\n",
    "- **Average: 0.93**\n",
    "\n",
    "### Sample 8:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 9:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 0.8\n",
    "- Correctness of Answer: 0.8\n",
    "- **Average: 0.87**\n",
    "\n",
    "### Sample 10:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 11:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 12:\n",
    "- Grammatical Correctness: 0.8\n",
    "- Coherence: 0.8\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 0.87**\n",
    "\n",
    "### Sample 13:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 0\n",
    "- **Average: 0.67**\n",
    "\n",
    "### Sample 14:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 15:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 16:\n",
    "- Grammatical Correctness: 0.8\n",
    "- Coherence: 0.8\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 0.87**\n",
    "\n",
    "### Sample 17:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 18:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 19:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 20:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 21:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 22:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 23:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 24:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 0.8\n",
    "- **Average: 0.93**\n",
    "\n",
    "### Sample 25:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "### Sample 26:\n",
    "- Grammatical Correctness: 1\n",
    "- Coherence: 1\n",
    "- Correctness of Answer: 1\n",
    "- **Average: 1**\n",
    "\n",
    "\n",
    "\n",
    "**Overall Average Score for All Samples:** 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.10637520896383895\n",
      "Rouge-L: 0.28829402465887405\n",
      "BERTScore: 0.8832031024826897\n",
      "Perplexity: 15.695914568724456\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def evaluate_model_performance(dataset, generated_responses):\n",
    "    # Initialize metrics and lists to save answers\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    bert_f1_scores = []\n",
    "    perplexity_scores = []\n",
    "\n",
    "    # Make sure you have the correct number of responses\n",
    "    assert len(dataset) == len(generated_responses), \"The number of generated responses must match the number of dataset entries\"\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        reference_answer = dataset[i]['output']\n",
    "        generated_answer = generated_responses[i]\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        bleu_score = corpus_bleu([generated_answer], [[reference_answer]])\n",
    "        bleu_score_normalized = bleu_score.score / 100.0\n",
    "        bleu_scores.append(bleu_score_normalized)\n",
    "        \n",
    "        rouge_l_scores.append(rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True).score(reference_answer, generated_answer)['rougeL'].fmeasure)\n",
    "        \n",
    "        bert_f1_scores.append(score([generated_answer], [reference_answer], lang='en')[2].mean().item())\n",
    "\n",
    "        # Calculate perplexity\n",
    "        # encodings = tokenizer(generated_answer, return_tensors='pt')\n",
    "        # with torch.no_grad():\n",
    "        #     outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "        #     loss = outputs.loss\n",
    "        #     perplexity = torch.exp(loss).item()\n",
    "        # perplexity_scores.append(perplexity)\n",
    "        # Check if generated_answer is not empty\n",
    "        if len(generated_answer) > 0:\n",
    "            encodings = tokenizer(generated_answer, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "                loss = outputs.loss\n",
    "                perplexity = torch.exp(loss).item()\n",
    "            perplexity_scores.append(perplexity)\n",
    "        else:\n",
    "            # Handle empty generated_answer, e.g., by appending a default value or skipping\n",
    "            perplexity_scores.append(0.0)\n",
    "\n",
    "\n",
    "    # Calculate average scores\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "    average_bert_f1 = sum(bert_f1_scores) / len(bert_f1_scores)\n",
    "    average_perplexity = sum(perplexity_scores) / len(perplexity_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f'BLEU: {average_bleu}')\n",
    "    print(f'Rouge-L: {average_rouge_l}')\n",
    "    print(f'BERTScore: {average_bert_f1}')\n",
    "    print(f'Perplexity: {average_perplexity}')\n",
    "\n",
    "    return average_bleu, average_rouge_l, average_bert_f1, average_perplexity\n",
    "\n",
    "average_bleu, average_rouge_l, average_bert_f1, average_perplexity = evaluate_model_performance(test_dataset, generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=10, fixed_beam_size=1, fixed_temperature=0.8:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=10, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:12<00:00, 36.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.07673658964472913\n",
      "Rouge-L: 0.24900730021602321\n",
      "BERTScore: 0.8739797782014918\n",
      "Perplexity: 21.311663486339427\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=25, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:03<00:00, 31.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.08314878179724342\n",
      "Rouge-L: 0.3034843794880543\n",
      "BERTScore: 0.8797346552213033\n",
      "Perplexity: 21.065436274917037\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=40, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:03<00:00, 31.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.08235278884122037\n",
      "Rouge-L: 0.27582336176884187\n",
      "BERTScore: 0.8798080439920779\n",
      "Perplexity: 25.632464974014848\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=75, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:03<00:00, 31.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.072184958782227\n",
      "Rouge-L: 0.24343805213106812\n",
      "BERTScore: 0.876941356394026\n",
      "Perplexity: 23.972018082936604\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "top_k_values = [10, 25, 40, 75]\n",
    "beam_sizes = [2, 4, 6, 8]\n",
    "temperatures = [0.25, 0.5, 0.7, 1.0]\n",
    "\n",
    "# Varying top_k while keeping beam_size and temperature fixed\n",
    "fixed_beam_size = 1\n",
    "fixed_temperature = 0.8\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    key = f\"top_k={top_k}, fixed_beam_size={fixed_beam_size}, fixed_temperature={fixed_temperature}\"\n",
    "    generated_output_2 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_2 = pipe(prompts, max_new_tokens=64, top_k=top_k, num_beams=fixed_beam_size, temperature=fixed_temperature, do_sample=True)\n",
    "\n",
    "        for result in results_2:\n",
    "            generated_text_2 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_2.append(generated_text_2)\n",
    "    \n",
    "    # generated_texts_2 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_2 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_2]\n",
    "    \n",
    "    average_bleu_2, average_rouge_l_2, average_bert_f1_2, average_perplexity_2 = evaluate_model_performance(test_dataset, generated_texts_2)\n",
    "\n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=2, fixed_temperature=0.8:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=2, fixed_temperature=0.8: 100%|██████████| 2/2 [01:11<00:00, 35.73s/it]\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.12227215271106785\n",
      "Rouge-L: 0.3037296181004547\n",
      "BERTScore: 0.8567791051334805\n",
      "Perplexity: 10.405672470728556\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=4, fixed_temperature=0.8: 100%|██████████| 2/2 [01:18<00:00, 39.26s/it]\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.11005320494944103\n",
      "Rouge-L: 0.2944300662000093\n",
      "BERTScore: 0.8545365907527782\n",
      "Perplexity: 11.13361930847168\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=6, fixed_temperature=0.8: 100%|██████████| 2/2 [01:31<00:00, 45.84s/it]\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.11375323415349615\n",
      "Rouge-L: 0.2953740508937465\n",
      "BERTScore: 0.8541421007227015\n",
      "Perplexity: 10.460258104183056\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=8, fixed_temperature=0.8: 100%|██████████| 2/2 [01:47<00:00, 53.84s/it]\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.11254707083161063\n",
      "Rouge-L: 0.2975668631316125\n",
      "BERTScore: 0.8577085490579959\n",
      "Perplexity: 10.476649734708998\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "fixed_top_k = 50\n",
    "\n",
    "for beam_size in beam_sizes:\n",
    "    key = f\"fixed_top_k={fixed_top_k}, beam_size={beam_size}, fixed_temperature={fixed_temperature}\"\n",
    "    generated_output_3 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_3 = pipe(prompts, max_new_tokens=64, top_k=fixed_top_k, num_beams=beam_size, temperature=fixed_temperature, do_sample=True)\n",
    "\n",
    "        for result in results_3:\n",
    "            generated_text_3 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_3.append(generated_text_3)\n",
    "    \n",
    "    # generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_3]\n",
    "\n",
    "    average_bleu_3, average_rouge_l_3, average_bert_f1_3, average_perplexity_3 = evaluate_model_performance(test_dataset, generated_texts_3)\n",
    "    \n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.25: 100%|██████████| 2/2 [01:03<00:00, 31.54s/it]\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.10193397174826552\n",
      "Rouge-L: 0.28625010684537366\n",
      "BERTScore: 0.8559713871390732\n",
      "Perplexity: 12.9054487546285\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.5: 100%|██████████| 2/2 [01:03<00:00, 31.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.08989354982086553\n",
      "Rouge-L: 0.2787655079688409\n",
      "BERTScore: 0.8827785739192257\n",
      "Perplexity: 18.191687919475413\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.7: 100%|██████████| 2/2 [01:03<00:00, 31.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.08830518129468479\n",
      "Rouge-L: 0.26620119722945434\n",
      "BERTScore: 0.8817243951338308\n",
      "Perplexity: 22.254915873209637\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=1.0: 100%|██████████| 2/2 [01:03<00:00, 31.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.08150223053665961\n",
      "Rouge-L: 0.24301592360628205\n",
      "BERTScore: 0.8716559299716243\n",
      "Perplexity: 25.44630449789542\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for temperature in temperatures:\n",
    "    key = f\"fixed_top_k={fixed_top_k}, fixed_beam_size={fixed_beam_size}, temperature={temperature}\"\n",
    "    generated_output_4 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_4 = pipe(prompts, max_new_tokens=64, top_k=fixed_top_k, num_beams=fixed_beam_size, temperature=temperature, do_sample=True)\n",
    "\n",
    "        for result in results_4:\n",
    "            generated_text_4 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_4.append(generated_text_4)\n",
    "    \n",
    "    # generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_4 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_4]\n",
    "\n",
    "    average_bleu_4, average_rouge_l_4, average_bert_f1_4, average_perplexity_4 = evaluate_model_performance(test_dataset, generated_texts_4)\n",
    "    \n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
