{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/gns938/nlp_hw1c/exploring_llm_hw/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "whole_dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_datasets = whole_dataset['train'].train_test_split(test_size=0.0005, seed=42)\n",
    "\n",
    "# Access the training and testing sets\n",
    "train_dataset = split_datasets['train']\n",
    "test_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.32s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "model_path = 'Phi-2-fine-tuned'\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text:  50%|█████     | 1/2 [00:50<00:50, 50.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What would be the best type of exercise for a person who has arthritis?\n",
      "\n",
      "### Response:\n",
      "Low-impact exercises such as swimming, walking, and cycling are great for people with arthritis. These exercises are gentle on the joints and can help to reduce pain and stiffness. Strength training exercises such as yoga and Pilates can also be beneficial for people with arthritis, as they can help to\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Calculate the atomic mass for lithium.\n",
      "\n",
      "### Response:\n",
      "The atomic mass for lithium is 6.94. This is calculated by adding the number of protons (1) and neutrons (6) in the lithium atom. The atomic mass of an element is the average mass of all the isotopes of that element, taking into account their relative\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the following binary code to ASCII characters.\n",
      "\n",
      "### Input:\n",
      "01110101 01100001 01110010 01101001 01110100\n",
      "\n",
      "### Response:\n",
      "The ASCII characters for the binary code is: uarit.\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Take this sentence and make it more descriptive: She was scared.\n",
      "\n",
      "### Input:\n",
      "She was scared.\n",
      "\n",
      "### Response:\n",
      "She was trembling with fear, her heart racing wildly and her breath coming in short, panicked gasps. She felt her palms begin to sweat and her stomach tying itself into knots.  She was scared.\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Brainstorm 3 possible ideas to promote the following product\n",
      "\n",
      "### Input:\n",
      "New motorized skateboard\n",
      "\n",
      "### Response:\n",
      "1. Offer a free trial for a limited time.\n",
      "2. Give away several skateboards in a raffle or sweepstakes.\n",
      "3. Host a promotional event featuring the skateboard doing stunts in various locations.\n",
      "4. Create engaging content on social media platforms highlighting the product's\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using imperative mood.\n",
      "\n",
      "### Input:\n",
      "Please pick up the book from the shelf.\n",
      "\n",
      "### Response:\n",
      "Pick up the book from the shelf.\n",
      "\n",
      "<noinput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe a picnic vacation where four people are travelling together.\n",
      "\n",
      "### Input:\n",
      "Four people are travelling together to a picnic vacation.\n",
      "\n",
      "### Response:\n",
      "The four friends decided to take a picnic vacation to the countryside. They packed their sandwiches and snacks, filled thermoses with hot tea, and wore their warmest coats. They spent their days exploring the incredible landscapes, and in the evenings they sat around a campfire singing and enjoying each other\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Explain the concept of Big Data and what it means for companies and customers.\n",
      "\n",
      "### Response:\n",
      "Big Data is a term used to describe large and complex data sets that are difficult to process and analyze using traditional methods. It is characterized by its volume, velocity, and variety, and is often used to gain insights into customer behavior, market trends, and other business-related information. Companies\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Analyze the given phrase in terms of its connotations.\n",
      "\n",
      "### Input:\n",
      "Wild goose chase\n",
      "\n",
      "### Response:\n",
      "The phrase \"wild goose chase\" is typically used to describe a futile and time-consuming endeavor. It can also connote a sense of futility, as trying to find something that one might never find is generally not seen as a worthwhile use of time and energy. Additionally, it implies\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List three potential sources for the following research problem.\n",
      "\n",
      "### Input:\n",
      "What are the benefits of online learning?\n",
      "\n",
      "### Response:\n",
      "1. Journal of Online Learning Research \n",
      "2. Harvard Business Review \n",
      "3. Educational Technology & Society\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Arrange the following countries in ascending order of GDP per capita.\n",
      "\n",
      "### Input:\n",
      "China, US, India, Japan\n",
      "\n",
      "### Response:\n",
      "India, China, Japan, US\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the boat depicted in the provided photo.\n",
      "\n",
      "### Input:\n",
      "Attached photo\n",
      "\n",
      "### Response:\n",
      "The boat in the photo is a sailboat. It has a tall mast with sails, a pointed bow, and a long, narrow hull. It is likely used for recreational sailing or racing.\n",
      "\n",
      "<noinput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Edit the grammar and punctuation of the given sentence.\n",
      "\n",
      "### Input:\n",
      "Humor is one of the most important ingredients of life\n",
      "\n",
      "### Response:\n",
      "Humor is one of the most important ingredients in life.\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Calculate the length of the hypotenuse in the following triangle.\n",
      "\n",
      "### Input:\n",
      "![alt text](https://www.onlinemathlearning.com/image-files/triangle-hypotenuse-length.png \"Right Triangle\")\n",
      "\n",
      "### Response:\n",
      "The length of the hypotenuse is approximately 8.66 units. This can be calculated using the Pythagorean Theorem, which states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. In this case, the hypotenuse\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How many instruments does a symphony orchestra typically have?\n",
      "\n",
      "### Response:\n",
      "A symphony orchestra typically has around 80-100 musicians, including string, woodwind, brass, and percussion instruments. Some orchestras may also include a harp, organ, or other instruments. The number of musicians can vary depending on the size of the orchestra and the type of music\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Summarize the phrase: \"GPT is a natural language processing architecture.\"\n",
      "\n",
      "### Response:\n",
      "GPT is a type of natural language processing architecture that is designed to understand and generate natural language. It is based on a deep learning model that is trained on large amounts of data and can be used for a variety of tasks such as text generation, question answering, and machine translation. G\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Tell me the name of the capital of France. Output the name.\n",
      "\n",
      "### Input:\n",
      "noinput\n",
      "\n",
      "### Response:\n",
      "Paris. Paris is the capital of France. It is located in the north-central part of the country and is the most populous city in France. It is also the capital of the Île-de-France region. Paris is known for its iconic landmarks such as the Eiff\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Brainstorm five questions that you could ask a customer to better understand their needs.\n",
      "\n",
      "### Response:\n",
      "1. What is your primary goal for using our product/service?\n",
      "2. What features do you need to achieve your goal?\n",
      "3. What challenges are you facing in achieving your goal?\n",
      "4. What are your expectations for the product/service?\n",
      "5. What are\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Brainstorm different ways to cook a specific type of food.\n",
      "\n",
      "### Input:\n",
      "Pork chops\n",
      "\n",
      "### Response:\n",
      "You can cook pork chops in a variety of ways. For instance, you can bake, fry, grill, or even sauté them. You can also season them with various herbs and spices to give them a unique flavor. Additionally, pork chops can be marinated to make them moist\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate an example of a customer service conversation involving the keyword \"shipping\".\n",
      "\n",
      "### Response:\n",
      "Customer: Hi, I'm trying to place an order but I'm not sure about the shipping options.\n",
      "\n",
      "Customer Service: Hi there! We offer a variety of shipping options, including standard, expedited, and express. Which one would you like to choose? \n",
      "\n",
      "Customer\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|██████████| 2/2 [01:11<00:00, 35.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is arithmetic mean and how to calculate it?\n",
      "\n",
      "### Response:\n",
      "The arithmetic mean is a measure of central tendency that is calculated by adding up all the values in a set of data and then dividing the sum by the number of values in the set. It is also known as the average. To calculate the arithmetic mean, you need to add up all the\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Describe a method for detecting spam emails.\n",
      "\n",
      "### Response:\n",
      "A method for detecting spam emails involves using a combination of techniques such as content analysis, machine learning, and natural language processing. Content analysis involves analyzing the content of the email to identify keywords and phrases that are commonly associated with spam. Machine learning algorithms can be used to identify patterns in the content\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Develop a computer program that adds two numbers.\n",
      "\n",
      "### Response:\n",
      "def add_two_numbers(a, b):\n",
      "    return a + b\n",
      "\n",
      "print(add_two_numbers(2, 3)) # Output: 5\n",
      "print(add_two_numbers(10, 20)) # Output: 30\n",
      "print(\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "How can one reduce the number of distractions when studying?\n",
      "\n",
      "### Response:\n",
      "One way to reduce distractions when studying is to create a dedicated study space. This space should be free from any distractions such as television, music, or other people. Additionally, it is important to set a timer and stick to a schedule. This will help to keep you focused and on task\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Construct an algorithm to solve the given problem.\n",
      "\n",
      "### Input:\n",
      "Problem: Calculate the sum of all multiples of 3 and 5 between 1 and 100\n",
      "\n",
      "### Response:\n",
      "Algorithm:\n",
      "1. Initialize Sum as 0.\n",
      "2. Set a variable i to 0.\n",
      "3. While i is less than or equal to 100, do the following: \n",
      "  a. If i is divisible by 5 or 3, add the value of\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Pick out the proper noun from this sentence:\n",
      "\n",
      "### Input:\n",
      "The government has implemented new regulations\n",
      "\n",
      "### Response:\n",
      "Government\n",
      "\n",
      "------\n",
      "Generated Text: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a list of interview questions for a software development role.\n",
      "\n",
      "### Response:\n",
      "1. What experience do you have in software development?\n",
      "2. What programming languages are you proficient in?\n",
      "3. How do you approach problem solving?\n",
      "4. What is your experience with agile development?\n",
      "5. How do you stay up to date with the latest technologies?\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=finetuned_model,\n",
    "    tokenizer=finetuned_tokenizer,\n",
    "    device=0,\n",
    "    # top_k=50,  # Set top_k to your desired value\n",
    "    # num_beams=5,  # Set beam_size to your desired value\n",
    "    # temperature=1  # Set temperature to your desired value\n",
    ")\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "num_examples = len(test_dataset)\n",
    "print(num_examples)\n",
    "total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "generated_output = []\n",
    "\n",
    "for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "    batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "    batch = test_dataset.select(batch_indices)\n",
    "    prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "    # print(prompts)\n",
    "    # Generate text for the batch\n",
    "    results = pipe(prompts, max_new_tokens=64)\n",
    "    \n",
    "    for result in results:\n",
    "        generated_text = result[0]['generated_text']\n",
    "        generated_output.append(generated_text)\n",
    "\n",
    "        # Uncomment the following lines if you want to print the prompts and generated text\n",
    "        prompt = prompts[results.index(result)]\n",
    "        # print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated Text: {generated_text}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "# generated_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Reference answer:\n",
      "For someone with arthritis, the best type of exercise would be low-impact activities like yoga, swimming, or walking. These exercises provide the benefits of exercise without exacerbating the symptoms of arthritis.\n",
      "\n",
      "Generated answer:\n",
      "Low-impact exercises such as swimming, walking, and cycling are great for people with arthritis. These exercises are gentle on the joints and can help to reduce pain and stiffness. Strength training exercises such as yoga and Pilates can also be beneficial for people with arthritis, as they can help to\n",
      "\n",
      "1\n",
      "Reference answer:\n",
      "The atomic mass for lithium is 6.941 u (units). This is determined from the number of protons and neutrons in the nucleus of a lithium atom, which is 3 protons and 4 neutrons. Each proton and neutron has a mass of 1.007 u, resulting in a total mass of 6.941 u.\n",
      "\n",
      "Generated answer:\n",
      "The atomic mass for lithium is 6.94. This is calculated by adding the number of protons (1) and neutrons (6) in the lithium atom. The atomic mass of an element is the average mass of all the isotopes of that element, taking into account their relative\n",
      "\n",
      "2\n",
      "Reference answer:\n",
      "The ASCII characters for the binary code is: wast.\n",
      "\n",
      "Generated answer:\n",
      "The ASCII characters for the binary code is: uarit.\n",
      "\n",
      "\n",
      "3\n",
      "Reference answer:\n",
      "She was trembling with fear, her heart racing wildly and her breath coming in short, panicked gasps. She felt her palms begin to sweat and her stomach tying itself into knots.  She was scared.\n",
      "\n",
      "Generated answer:\n",
      "She was trembling with fear, her heart racing wildly and her breath coming in short, panicked gasps. She felt her palms begin to sweat and her stomach tying itself into knots.  She was scared.\n",
      "\n",
      "\n",
      "4\n",
      "Reference answer:\n",
      "1. Offer a free trial for a limited time.\n",
      "2. Give away several skateboards in a raffle or sweepstakes.\n",
      "3. Host a promotional event featuring the skateboard doing stunts in various locations.\n",
      "\n",
      "Generated answer:\n",
      "1. Offer a free trial for a limited time.\n",
      "2. Give away several skateboards in a raffle or sweepstakes.\n",
      "3. Host a promotional event featuring the skateboard doing stunts in various locations.\n",
      "4. Create engaging content on social media platforms highlighting the product's\n",
      "\n",
      "5\n",
      "Reference answer:\n",
      "Pick up the book from the shelf.\n",
      "\n",
      "Generated answer:\n",
      "Pick up the book from the shelf.\n",
      "\n",
      "<noinput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "Reference answer:\n",
      "The four friends decided to take a picnic vacation to the countryside. They packed their sandwiches and snacks, filled thermoses with hot tea, and wore their warmest coats. They spent their days exploring the incredible landscapes, and in the evenings they sat around a campfire singing and enjoying each other's company. They spent their days immersed in nature, getting to know the hidden gems of the place. When they returned home, they were filled with beautiful memories that would last a lifetime.\n",
      "\n",
      "Generated answer:\n",
      "The four friends decided to take a picnic vacation to the countryside. They packed their sandwiches and snacks, filled thermoses with hot tea, and wore their warmest coats. They spent their days exploring the incredible landscapes, and in the evenings they sat around a campfire singing and enjoying each other\n",
      "\n",
      "7\n",
      "Reference answer:\n",
      "Big Data is the massive volume of data that is generated from companies’ interactions with customers, both online and offline. This data includes customer data such as age, gender, location, behaviors, preferences, and transactions. This data enables companies to gain insights into customer behavior and to develop data-driven marketing strategies. For customers, Big Data provides personalized and tailored experiences, as companies can use the data they have collected to offer more targeted content and customized products or services.\n",
      "\n",
      "Generated answer:\n",
      "Big Data is a term used to describe large and complex data sets that are difficult to process and analyze using traditional methods. It is characterized by its volume, velocity, and variety, and is often used to gain insights into customer behavior, market trends, and other business-related information. Companies\n",
      "\n",
      "8\n",
      "Reference answer:\n",
      "The phrase \"wild goose chase\" is typically used to describe a futile and time-consuming endeavor. It can also connote a sense of futility, as trying to find something that one might never find is generally not seen as a worthwhile use of time and energy. Additionally, it implies a sense of confusion, as the phrase itself refers to a scavenger hunt with a disappearing prize.\n",
      "\n",
      "Generated answer:\n",
      "The phrase \"wild goose chase\" is typically used to describe a futile and time-consuming endeavor. It can also connote a sense of futility, as trying to find something that one might never find is generally not seen as a worthwhile use of time and energy. Additionally, it implies\n",
      "\n",
      "9\n",
      "Reference answer:\n",
      "1. Journal of Online Learning Research \n",
      "2. Harvard Business Review \n",
      "3. Educational Technology & Society\n",
      "\n",
      "Generated answer:\n",
      "1. Journal of Online Learning Research \n",
      "2. Harvard Business Review \n",
      "3. Educational Technology & Society\n",
      "\n",
      "\n",
      "10\n",
      "Reference answer:\n",
      "India, China, Japan, US\n",
      "\n",
      "Generated answer:\n",
      "India, China, Japan, US\n",
      "\n",
      "\n",
      "11\n",
      "Reference answer:\n",
      "The boat in the photo is a sailboat.\n",
      "\n",
      "Generated answer:\n",
      "The boat in the photo is a sailboat. It has a tall mast with sails, a pointed bow, and a long, narrow hull. It is likely used for recreational sailing or racing.\n",
      "\n",
      "<noinput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "\n",
      "12\n",
      "Reference answer:\n",
      "Humor is one of the most important ingredients in life.\n",
      "\n",
      "Generated answer:\n",
      "Humor is one of the most important ingredients in life.\n",
      "\n",
      "\n",
      "13\n",
      "Reference answer:\n",
      "The length of the hypotenuse in the triangle is 5.  To calculate this, we use the Pythagorean theorem, which states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. In this case, the other two sides have lengths of 3 and 4, so the hypotenuse must have a length of 5.\n",
      "\n",
      "Generated answer:\n",
      "The length of the hypotenuse is approximately 8.66 units. This can be calculated using the Pythagorean Theorem, which states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. In this case, the hypotenuse\n",
      "\n",
      "14\n",
      "Reference answer:\n",
      "A symphony orchestra typically has anywhere from 60 to 100 different instruments in its ensemble.\n",
      "\n",
      "Generated answer:\n",
      "A symphony orchestra typically has around 80-100 musicians, including string, woodwind, brass, and percussion instruments. Some orchestras may also include a harp, organ, or other instruments. The number of musicians can vary depending on the size of the orchestra and the type of music\n",
      "\n",
      "15\n",
      "Reference answer:\n",
      "GPT is a deep learning-based natural language processing architecture designed to generate highly realistic and human-like text and speech.\n",
      "\n",
      "Generated answer:\n",
      "GPT is a type of natural language processing architecture that is designed to understand and generate natural language. It is based on a deep learning model that is trained on large amounts of data and can be used for a variety of tasks such as text generation, question answering, and machine translation. G\n",
      "\n",
      "16\n",
      "Reference answer:\n",
      "Paris\n",
      "\n",
      "Generated answer:\n",
      "Paris. Paris is the capital of France. It is located in the north-central part of the country and is the most populous city in France. It is also the capital of the Île-de-France region. Paris is known for its iconic landmarks such as the Eiff\n",
      "\n",
      "17\n",
      "Reference answer:\n",
      "Here are five questions to better understand a customer's needs:\n",
      "\n",
      "1. What are your main objectives for this project? \n",
      "2. What have you tried before that didn't work?\n",
      "3. What is the main goal you're hoping to achieve? \n",
      "4. What resources do you currently have to achieve your goals? \n",
      "5. What portion of the work do you plan to outsource, and what will be done in-house?\n",
      "\n",
      "Generated answer:\n",
      "1. What is your primary goal for using our product/service?\n",
      "2. What features do you need to achieve your goal?\n",
      "3. What challenges are you facing in achieving your goal?\n",
      "4. What are your expectations for the product/service?\n",
      "5. What are\n",
      "\n",
      "18\n",
      "Reference answer:\n",
      "You can cook pork chops in a variety of ways. For instance, you can bake, fry, grill, or even sauté them. You can also season them with various herbs and spices to give them a unique flavor. Additionally, pork chops can be marinated to make them moist and tender.\n",
      "\n",
      "Generated answer:\n",
      "You can cook pork chops in a variety of ways. For instance, you can bake, fry, grill, or even sauté them. You can also season them with various herbs and spices to give them a unique flavor. Additionally, pork chops can be marinated to make them moist\n",
      "\n",
      "19\n",
      "Reference answer:\n",
      "User: Hi, I'm wondering when can I expect my order to arrive.\n",
      "Bot: We offer free standard shipping on all orders and the expected delivery date should have been provided to you with the shipment confirmation email. If you need to double check, please provide me with the order details and I'll check the status of your order.\n",
      "\n",
      "Generated answer:\n",
      "Customer: Hi, I'm trying to place an order but I'm not sure about the shipping options.\n",
      "\n",
      "Customer Service: Hi there! We offer a variety of shipping options, including standard, expedited, and express. Which one would you like to choose? \n",
      "\n",
      "Customer\n",
      "\n",
      "20\n",
      "Reference answer:\n",
      "The arithmetic mean is the sum of all the values in a set of data, divided by the number of values in the set. For example, if you have six values of 5, 2, 8, 4, 9, and 7, the arithmetic mean would be (5 + 2 + 8 + 4 + 9 + 7) / 6 = 6. \n",
      "\n",
      "To calculate the arithmetic mean for a set of data, you simply add up all the values and divide by the number of values in the set.\n",
      "\n",
      "Generated answer:\n",
      "The arithmetic mean is a measure of central tendency that is calculated by adding up all the values in a set of data and then dividing the sum by the number of values in the set. It is also known as the average. To calculate the arithmetic mean, you need to add up all the\n",
      "\n",
      "21\n",
      "Reference answer:\n",
      "One method for detecting spam emails is to use a machine learning model that is trained on examples of known spam emails. The model can then analyze new emails for patterns that are indicative of spam emails, such as the presence of certain keywords. Additionally, filters can be used to block emails from suspicious senders and detect emails with suspicious content.\n",
      "\n",
      "Generated answer:\n",
      "A method for detecting spam emails involves using a combination of techniques such as content analysis, machine learning, and natural language processing. Content analysis involves analyzing the content of the email to identify keywords and phrases that are commonly associated with spam. Machine learning algorithms can be used to identify patterns in the content\n",
      "\n",
      "22\n",
      "Reference answer:\n",
      "The program should take two integers as input and return their sum as output. It should be written in a programming language such as Python, Java, or Javascript.\n",
      "\n",
      "// Sample code:\n",
      "\n",
      "// Start of program\n",
      "int a;\n",
      "int b;\n",
      "int sum;\n",
      "\n",
      "a = input(); // read first number from the user\n",
      "b = input(); // read second number from the user\n",
      "sum = a + b; // calculate sum\n",
      "print(sum); // print the output\n",
      "// End of program\n",
      "\n",
      "Generated answer:\n",
      "def add_two_numbers(a, b):\n",
      "    return a + b\n",
      "\n",
      "print(add_two_numbers(2, 3)) # Output: 5\n",
      "print(add_two_numbers(10, 20)) # Output: 30\n",
      "print(\n",
      "\n",
      "23\n",
      "Reference answer:\n",
      "To reduce the number of distractions when studying, it is best to have a dedicated workspace that is free from clutter and other distractions such as noise and technology. It is also important to set a schedule and stick to it, as well as take regular breaks. Additionally, it is beneficial to practice meditation or deep breathing to stay focused and clear the mind.\n",
      "\n",
      "Generated answer:\n",
      "One way to reduce distractions when studying is to create a dedicated study space. This space should be free from any distractions such as television, music, or other people. Additionally, it is important to set a timer and stick to a schedule. This will help to keep you focused and on task\n",
      "\n",
      "24\n",
      "Reference answer:\n",
      "Algorithm:\n",
      "1. Initialize Sum as 0.\n",
      "2. Set a variable i to 0.\n",
      "3. While i is less than or equal to 100, do the following: \n",
      "  a. If i is divisible by 5 or 3, add the value of i to Sum.\n",
      "  b. Increment i by 1.\n",
      "4. Return the value of Sum.\n",
      "\n",
      "Generated answer:\n",
      "Algorithm:\n",
      "1. Initialize Sum as 0.\n",
      "2. Set a variable i to 0.\n",
      "3. While i is less than or equal to 100, do the following: \n",
      "  a. If i is divisible by 5 or 3, add the value of\n",
      "\n",
      "25\n",
      "Reference answer:\n",
      "Government\n",
      "\n",
      "Generated answer:\n",
      "Government\n",
      "\n",
      "\n",
      "26\n",
      "Reference answer:\n",
      "• What led you to pursue a career in software development?\n",
      "• Describe your experience with developing software applications.\n",
      "• What challenges have you faced in software development and how did you overcome them?\n",
      "• What techniques do you use to debug code?\n",
      "• How do you keep yourself up-to-date with the latest software development technologies and trends?\n",
      "• What type of software development project are you most proud of and why?\n",
      "• How do you ensure code quality and performance?\n",
      "• Describe your experience with Agile/Scrum methodology.\n",
      "• Talk about a time when you had to take initiative to solve a problem during a software development project.\n",
      "• Describe an instance when you had to collaborate with a team member to develop a complex software feature.\n",
      "• How do you handle difficult stakeholders?\n",
      "• Describe a difficult decision you made while working on a software development project.\n",
      "• Talk about a time when you had to adjust to a change in requirements and how you handled it.\n",
      "\n",
      "Generated answer:\n",
      "1. What experience do you have in software development?\n",
      "2. What programming languages are you proficient in?\n",
      "3. How do you approach problem solving?\n",
      "4. What is your experience with agile development?\n",
      "5. How do you stay up to date with the latest technologies?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def human_evaluation_print(dataset, generated_responses):\n",
    "\n",
    "    # Make sure you have the correct number of responses\n",
    "    assert len(dataset) == len(generated_responses), \"The number of generated responses must match the number of dataset entries\"\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        reference_answer = dataset[i]['output']\n",
    "        generated_answer = generated_responses[i]\n",
    "        print(i)\n",
    "        print(f\"Reference answer:\\n{reference_answer}\\n\\nGenerated answer:\\n{generated_answer}\\n\")\n",
    "\n",
    "human_evaluation_print(test_dataset, generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scores for Each Sample:**\n",
    "\n",
    "1. **Sample 1 (Arthritis Exercise):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 0.8 (The generated answer mentions cycling, which is not typically considered a low-impact exercise for arthritis.)\n",
    "   - **Average: 0.93**\n",
    "\n",
    "2. **Sample 2 (Atomic Mass of Lithium):**\n",
    "   - Grammatical Correctness: 0.8 (The generated answer has a repetitive sentence.)\n",
    "   - Coherence: 0.8 (The generated answer is not coherent due to the repetition.)\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 0.87**\n",
    "\n",
    "3. **Sample 3 (ASCII Characters):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 0 (The generated answer is incorrect.)\n",
    "   - **Average: 0.67**\n",
    "\n",
    "4. **Sample 4 (Fear Description):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 1**\n",
    "\n",
    "5. **Sample 5 (Skateboard Promotion):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 0.9 (The generated answer adds an incomplete sentence about social media.)\n",
    "   - **Average: 0.97**\n",
    "\n",
    "6. **Sample 6 (Picnic Vacation):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 0.8 (The generated answer is cut off, making it less coherent.)\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 0.93**\n",
    "\n",
    "7. **Sample 7 (Big Data):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 0.8 (The generated answer is incomplete and less coherent.)\n",
    "   - Correctness of Answer: 0.8 (The generated answer misses the aspect of customer interaction.)\n",
    "   - **Average: 0.87**\n",
    "\n",
    "8. **Sample 8 (Wild Goose Chase):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 1**\n",
    "\n",
    "9. **Sample 9 (Journals):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 1**\n",
    "\n",
    "10. **Sample 10 (Countries):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "11. **Sample 11 (Sailboat):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer adds unnecessary details.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "12. **Sample 12 (Humor Importance):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "13. **Sample 13 (Hypotenuse Length):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0 (The generated answer provides an incorrect value.)\n",
    "    - **Average: 0.67**\n",
    "\n",
    "14. **Sample 14 (Symphony Orchestra):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer adds unnecessary details.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "15. **Sample 15 (GPT Description):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "16. **Sample 16 (Paris):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer adds unnecessary details.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "17. **Sample 17 (Understanding Customer's Needs):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer is less specific and comprehensive.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "18. **Sample 18 (Cooking Pork Chops):**\n",
    "    - Grammatically Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "19. **Sample 19 (Customer Service):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 0.8 (The generated answer is a bit disjointed.)\n",
    "    - Correctness of Answer: 0.8 (The generated answer doesn't address the original query about order arrival.)\n",
    "    - **Average: 0.87**\n",
    "\n",
    "20. **Sample 20 (Arithmetic Mean):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "21. **Sample 21 (Detecting Spam Emails):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "22. **Sample 22 (Sum Program):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "23. **Sample 23 (Reducing Distractions):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "24. **Sample 24 (Sum Algorithm):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "25. **Sample 25 (Government):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "26. **Sample 26 (Software Development Questions):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer is less comprehensive.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "**Overall Average Score:** 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 38.65230941540421\n",
      "Rouge-L: 0.577791674336474\n",
      "BERTScore: 0.927201858273259\n",
      "Perplexity: 21.4041663452431\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def evaluate_model_performance(dataset, generated_responses):\n",
    "    # Initialize metrics and lists to save answers\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    bert_f1_scores = []\n",
    "    perplexity_scores = []\n",
    "\n",
    "    # Make sure you have the correct number of responses\n",
    "    assert len(dataset) == len(generated_responses), \"The number of generated responses must match the number of dataset entries\"\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        reference_answer = dataset[i]['output']\n",
    "        generated_answer = generated_responses[i]\n",
    "        \n",
    "        bleu_scores.append(corpus_bleu([generated_answer], [[reference_answer]]).score)\n",
    "        rouge_l_scores.append(rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True).score(reference_answer, generated_answer)['rougeL'].fmeasure)\n",
    "        bert_f1_scores.append(score([generated_answer], [reference_answer], lang='en')[2].mean().item())\n",
    "\n",
    "        # Calculate perplexity\n",
    "        encodings = tokenizer(generated_answer, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()\n",
    "        perplexity_scores.append(perplexity)\n",
    "\n",
    "    # Calculate average scores\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "    average_bert_f1 = sum(bert_f1_scores) / len(bert_f1_scores)\n",
    "    average_perplexity = sum(perplexity_scores) / len(perplexity_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f'BLEU: {average_bleu}')\n",
    "    print(f'Rouge-L: {average_rouge_l}')\n",
    "    print(f'BERTScore: {average_bert_f1}')\n",
    "    print(f'Perplexity: {average_perplexity}')\n",
    "\n",
    "    return average_bleu, average_rouge_l, average_bert_f1, average_perplexity\n",
    "\n",
    "average_bleu, average_rouge_l, average_bert_f1, average_perplexity = evaluate_model_performance(test_dataset, generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=10, fixed_beam_size=1, fixed_temperature=0.8:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=10, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:04<00:00, 32.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 37.63149152914763\n",
      "Rouge-L: 0.5559819586921096\n",
      "BERTScore: 0.9282636002258018\n",
      "Perplexity: 19.96817938486735\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=25, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:04<00:00, 32.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 36.18796545255251\n",
      "Rouge-L: 0.5396540050324894\n",
      "BERTScore: 0.9271815926940353\n",
      "Perplexity: 19.60500263284754\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=40, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:05<00:00, 32.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 36.687082103450734\n",
      "Rouge-L: 0.5515713485611292\n",
      "BERTScore: 0.9291376206609938\n",
      "Perplexity: 20.79656198289659\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=75, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:04<00:00, 32.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 36.34214353756424\n",
      "Rouge-L: 0.5405646073525433\n",
      "BERTScore: 0.9239199912106549\n",
      "Perplexity: 23.381509339367902\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "top_k_values = [10, 25, 40, 75]\n",
    "beam_sizes = [2, 4, 6, 8]\n",
    "temperatures = [0.25, 0.5, 0.7, 1.0]\n",
    "\n",
    "# Varying top_k while keeping beam_size and temperature fixed\n",
    "fixed_beam_size = 1\n",
    "fixed_temperature = 0.8\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    key = f\"top_k={top_k}, fixed_beam_size={fixed_beam_size}, fixed_temperature={fixed_temperature}\"\n",
    "    generated_output_2 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_2 = pipe(prompts, max_new_tokens=64, top_k=top_k, num_beams=fixed_beam_size, temperature=fixed_temperature, do_sample=True)\n",
    "\n",
    "        for result in results_2:\n",
    "            generated_text_2 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_2.append(generated_text_2)\n",
    "    \n",
    "    # generated_texts_2 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_2 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_2]\n",
    "    \n",
    "    average_bleu_2, average_rouge_l_2, average_bert_f1_2, average_perplexity_2 = evaluate_model_performance(test_dataset, generated_texts_2)\n",
    "\n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=2, fixed_temperature=0.8:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=2, fixed_temperature=0.8: 100%|██████████| 2/2 [01:07<00:00, 33.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 39.67188808929785\n",
      "Rouge-L: 0.5848735498976347\n",
      "BERTScore: 0.9295942385991415\n",
      "Perplexity: 20.6656516039813\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=4, fixed_temperature=0.8: 100%|██████████| 2/2 [01:10<00:00, 35.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 42.57277846736364\n",
      "Rouge-L: 0.5977791788493919\n",
      "BERTScore: 0.9361406454333553\n",
      "Perplexity: 21.49953254063924\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=6, fixed_temperature=0.8: 100%|██████████| 2/2 [01:14<00:00, 37.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 38.98229855918357\n",
      "Rouge-L: 0.5742071191754885\n",
      "BERTScore: 0.9285258143036453\n",
      "Perplexity: 21.440537523340296\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=8, fixed_temperature=0.8: 100%|██████████| 2/2 [01:16<00:00, 38.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 39.768879848971125\n",
      "Rouge-L: 0.5881864172496409\n",
      "BERTScore: 0.931098425829852\n",
      "Perplexity: 20.473666703259504\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "fixed_top_k = 50\n",
    "\n",
    "for beam_size in beam_sizes:\n",
    "    key = f\"fixed_top_k={fixed_top_k}, beam_size={beam_size}, fixed_temperature={fixed_temperature}\"\n",
    "    generated_output_3 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_3 = pipe(prompts, max_new_tokens=64, top_k=fixed_top_k, num_beams=beam_size, temperature=fixed_temperature, do_sample=True)\n",
    "\n",
    "        for result in results_3:\n",
    "            generated_text_3 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_3.append(generated_text_3)\n",
    "    \n",
    "    # generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_3]\n",
    "\n",
    "    average_bleu_3, average_rouge_l_3, average_bert_f1_3, average_perplexity_3 = evaluate_model_performance(test_dataset, generated_texts_3)\n",
    "    \n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.25:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.25: 100%|██████████| 2/2 [01:04<00:00, 32.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 38.79935954940628\n",
      "Rouge-L: 0.5841967669626138\n",
      "BERTScore: 0.9279305647920679\n",
      "Perplexity: 21.20727077236882\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.5: 100%|██████████| 2/2 [01:04<00:00, 32.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 37.18471176330375\n",
      "Rouge-L: 0.5560767714169326\n",
      "BERTScore: 0.9256197456960324\n",
      "Perplexity: 22.537801371680366\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.7: 100%|██████████| 2/2 [01:03<00:00, 31.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 36.2094992745774\n",
      "Rouge-L: 0.5576470343600868\n",
      "BERTScore: 0.9268559173301414\n",
      "Perplexity: 22.557431839130544\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=1.0: 100%|██████████| 2/2 [01:06<00:00, 33.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 33.941399942768655\n",
      "Rouge-L: 0.5316848877072834\n",
      "BERTScore: 0.9161695131549129\n",
      "Perplexity: 25.383931513185853\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for temperature in temperatures:\n",
    "    key = f\"fixed_top_k={fixed_top_k}, fixed_beam_size={fixed_beam_size}, temperature={temperature}\"\n",
    "    generated_output_4 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_4 = pipe(prompts, max_new_tokens=64, top_k=fixed_top_k, num_beams=fixed_beam_size, temperature=temperature, do_sample=True)\n",
    "\n",
    "        for result in results_4:\n",
    "            generated_text_4 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_4.append(generated_text_4)\n",
    "    \n",
    "    # generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_4 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_4]\n",
    "\n",
    "    average_bleu_4, average_rouge_l_4, average_bert_f1_4, average_perplexity_4 = evaluate_model_performance(test_dataset, generated_texts_4)\n",
    "    \n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.kaggle.com/code/kingabzpro/fine-tuning-phi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
