{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "whole_dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_datasets = whole_dataset['train'].train_test_split(test_size=0.0005, seed=42)\n",
    "\n",
    "# Access the training and testing sets\n",
    "train_dataset = split_datasets['train']\n",
    "test_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.set_verbosity(logging.CRITICAL)\n",
    "model_path = 'Phi-2-fine-tuned'\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=finetuned_model,\n",
    "    tokenizer=finetuned_tokenizer,\n",
    "    device=0,\n",
    "    # top_k=50,  # Set top_k to your desired value\n",
    "    # num_beams=5,  # Set beam_size to your desired value\n",
    "    # temperature=1  # Set temperature to your desired value\n",
    ")\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "num_examples = len(test_dataset)\n",
    "print(num_examples)\n",
    "total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "generated_output = []\n",
    "\n",
    "for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "    batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "    batch = test_dataset.select(batch_indices)\n",
    "    prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "    # print(prompts)\n",
    "    # Generate text for the batch\n",
    "    results = pipe(prompts, max_new_tokens=64)\n",
    "    \n",
    "    for result in results:\n",
    "        generated_text = result[0]['generated_text']\n",
    "        generated_output.append(generated_text)\n",
    "\n",
    "        # Uncomment the following lines if you want to print the prompts and generated text\n",
    "        prompt = prompts[results.index(result)]\n",
    "        # print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated Text: {generated_text}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "# generated_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Reference answer:\n",
      "For someone with arthritis, the best type of exercise would be low-impact activities like yoga, swimming, or walking. These exercises provide the benefits of exercise without exacerbating the symptoms of arthritis.\n",
      "\n",
      "Generated answer:\n",
      "Low-impact exercises such as swimming, cycling, and walking are great for people with arthritis. These exercises are gentle on the joints and can help to reduce pain and stiffness. Strength training exercises such as yoga and Pilates can also be beneficial for people with arthritis, as they can help to\n",
      "\n",
      "1\n",
      "Reference answer:\n",
      "The atomic mass for lithium is 6.941 u (units). This is determined from the number of protons and neutrons in the nucleus of a lithium atom, which is 3 protons and 4 neutrons. Each proton and neutron has a mass of 1.007 u, resulting in a total mass of 6.941 u.\n",
      "\n",
      "Generated answer:\n",
      "The atomic mass for lithium is 6.94. This is calculated by adding the number of protons (3) and the number of neutrons (3) in the lithium atom. The atomic mass of lithium is 6.94. This is the same as the atomic mass of lithium on\n",
      "\n",
      "2\n",
      "Reference answer:\n",
      "The ASCII characters for the binary code is: wast.\n",
      "\n",
      "Generated answer:\n",
      "The ASCII characters for the binary code is: uarit.\n",
      "\n",
      "\n",
      "3\n",
      "Reference answer:\n",
      "She was trembling with fear, her heart racing wildly and her breath coming in short, panicked gasps. She felt her palms begin to sweat and her stomach tying itself into knots.  She was scared.\n",
      "\n",
      "Generated answer:\n",
      "She was trembling with fear, her heart racing wildly and her breath coming in short, panicked gasps. She felt her palms begin to sweat and her stomach tying itself into knots.  She was scared.\n",
      "\n",
      "\n",
      "4\n",
      "Reference answer:\n",
      "1. Offer a free trial for a limited time.\n",
      "2. Give away several skateboards in a raffle or sweepstakes.\n",
      "3. Host a promotional event featuring the skateboard doing stunts in various locations.\n",
      "\n",
      "Generated answer:\n",
      "1. Offer a free trial for a limited time.\n",
      "2. Give away several skateboards in a raffle or sweepstakes.\n",
      "3. Host a promotional event featuring the skateboard doing stunts in various locations.\n",
      "4. Create engaging content on social media platforms highlighting the product's\n",
      "\n",
      "5\n",
      "Reference answer:\n",
      "Pick up the book from the shelf.\n",
      "\n",
      "Generated answer:\n",
      "Pick up the book from the shelf.\n",
      "\n",
      "<noinput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "Reference answer:\n",
      "The four friends decided to take a picnic vacation to the countryside. They packed their sandwiches and snacks, filled thermoses with hot tea, and wore their warmest coats. They spent their days exploring the incredible landscapes, and in the evenings they sat around a campfire singing and enjoying each other's company. They spent their days immersed in nature, getting to know the hidden gems of the place. When they returned home, they were filled with beautiful memories that would last a lifetime.\n",
      "\n",
      "Generated answer:\n",
      "The four friends decided to take a picnic vacation to the countryside. They packed their sandwiches and snacks, filled thermoses with hot tea, and wore their warmest coats. They spent their days exploring the incredible landscapes, and in the evenings they sat around a campfire singing and enjoying each other\n",
      "\n",
      "7\n",
      "Reference answer:\n",
      "Big Data is the massive volume of data that is generated from companies’ interactions with customers, both online and offline. This data includes customer data such as age, gender, location, behaviors, preferences, and transactions. This data enables companies to gain insights into customer behavior and to develop data-driven marketing strategies. For customers, Big Data provides personalized and tailored experiences, as companies can use the data they have collected to offer more targeted content and customized products or services.\n",
      "\n",
      "Generated answer:\n",
      "Big Data is a term used to describe large and complex data sets that are difficult to process and analyze using traditional methods. It is characterized by its volume, velocity, and variety, and is often used to gain insights into customer behavior, market trends, and other business-related information. Companies\n",
      "\n",
      "8\n",
      "Reference answer:\n",
      "The phrase \"wild goose chase\" is typically used to describe a futile and time-consuming endeavor. It can also connote a sense of futility, as trying to find something that one might never find is generally not seen as a worthwhile use of time and energy. Additionally, it implies a sense of confusion, as the phrase itself refers to a scavenger hunt with a disappearing prize.\n",
      "\n",
      "Generated answer:\n",
      "The phrase \"wild goose chase\" is typically used to describe a futile and time-consuming endeavor. It can also connote a sense of futility, as trying to find something that one might never find is generally not seen as a worthwhile use of time and energy. Additionally, it implies\n",
      "\n",
      "9\n",
      "Reference answer:\n",
      "1. Journal of Online Learning Research \n",
      "2. Harvard Business Review \n",
      "3. Educational Technology & Society\n",
      "\n",
      "Generated answer:\n",
      "1. Journal of Online Learning Research \n",
      "2. Harvard Business Review \n",
      "3. Educational Technology & Society\n",
      "\n",
      "\n",
      "10\n",
      "Reference answer:\n",
      "India, China, Japan, US\n",
      "\n",
      "Generated answer:\n",
      "India, China, Japan, US\n",
      "\n",
      "\n",
      "11\n",
      "Reference answer:\n",
      "The boat in the photo is a sailboat.\n",
      "\n",
      "Generated answer:\n",
      "The boat in the photo is a sailboat. It has a tall mast with sails, a pointed bow, and a long, narrow hull. It is likely used for recreational sailing or racing.\n",
      "\n",
      "<noinput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "<nooutput>\n",
      "\n",
      "\n",
      "12\n",
      "Reference answer:\n",
      "Humor is one of the most important ingredients in life.\n",
      "\n",
      "Generated answer:\n",
      "Humor is one of the most important ingredients in life.\n",
      "\n",
      "\n",
      "13\n",
      "Reference answer:\n",
      "The length of the hypotenuse in the triangle is 5.  To calculate this, we use the Pythagorean theorem, which states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. In this case, the other two sides have lengths of 3 and 4, so the hypotenuse must have a length of 5.\n",
      "\n",
      "Generated answer:\n",
      "The length of the hypotenuse is 5.83 units. This can be calculated using the Pythagorean Theorem, which states that the square of the hypotenuse is equal to the sum of the squares of the other two sides. Therefore, the length of the hypotenuse\n",
      "\n",
      "14\n",
      "Reference answer:\n",
      "A symphony orchestra typically has anywhere from 60 to 100 different instruments in its ensemble.\n",
      "\n",
      "Generated answer:\n",
      "A symphony orchestra typically has around 80-100 musicians, including string, woodwind, brass, and percussion instruments. Some orchestras may also have a harp, organ, or other instruments. The number of musicians can vary depending on the size of the orchestra and the type of music\n",
      "\n",
      "15\n",
      "Reference answer:\n",
      "GPT is a deep learning-based natural language processing architecture designed to generate highly realistic and human-like text and speech.\n",
      "\n",
      "Generated answer:\n",
      "GPT is a type of natural language processing architecture that is designed to understand and generate human-like text. It is based on a deep learning model that is trained on large amounts of text data. GPT is capable of understanding context and generating coherent and accurate responses to natural language queries.\n",
      "\n",
      "16\n",
      "Reference answer:\n",
      "Paris\n",
      "\n",
      "Generated answer:\n",
      "Paris. Paris is the capital of France. It is located in the north-central part of the country and is the largest city in France. It is also the capital of the Île-de-France region. Paris is known for its iconic landmarks such as the Eiffel\n",
      "\n",
      "17\n",
      "Reference answer:\n",
      "Here are five questions to better understand a customer's needs:\n",
      "\n",
      "1. What are your main objectives for this project? \n",
      "2. What have you tried before that didn't work?\n",
      "3. What is the main goal you're hoping to achieve? \n",
      "4. What resources do you currently have to achieve your goals? \n",
      "5. What portion of the work do you plan to outsource, and what will be done in-house?\n",
      "\n",
      "Generated answer:\n",
      "1. What are your main goals for this project?\n",
      "2. What challenges have you encountered so far?\n",
      "3. What resources do you have available to you?\n",
      "4. What is the timeline for this project?\n",
      "5. What is the budget for this project?\n",
      "6\n",
      "\n",
      "18\n",
      "Reference answer:\n",
      "You can cook pork chops in a variety of ways. For instance, you can bake, fry, grill, or even sauté them. You can also season them with various herbs and spices to give them a unique flavor. Additionally, pork chops can be marinated to make them moist and tender.\n",
      "\n",
      "Generated answer:\n",
      "You can cook pork chops in a variety of ways. For instance, you can bake, fry, grill, or even sauté them. You can also season them with various herbs and spices to give them a unique flavor. Additionally, pork chops can be marinated to make them moist\n",
      "\n",
      "19\n",
      "Reference answer:\n",
      "User: Hi, I'm wondering when can I expect my order to arrive.\n",
      "Bot: We offer free standard shipping on all orders and the expected delivery date should have been provided to you with the shipment confirmation email. If you need to double check, please provide me with the order details and I'll check the status of your order.\n",
      "\n",
      "Generated answer:\n",
      "Customer: Hi, I'm trying to place an order but I'm not sure about the shipping options.\n",
      "\n",
      "Customer Service: Hi there! We offer a variety of shipping options, including standard, express, and priority. Which one would you like to choose?\n",
      "\n",
      "Customer: I\n",
      "\n",
      "20\n",
      "Reference answer:\n",
      "The arithmetic mean is the sum of all the values in a set of data, divided by the number of values in the set. For example, if you have six values of 5, 2, 8, 4, 9, and 7, the arithmetic mean would be (5 + 2 + 8 + 4 + 9 + 7) / 6 = 6. \n",
      "\n",
      "To calculate the arithmetic mean for a set of data, you simply add up all the values and divide by the number of values in the set.\n",
      "\n",
      "Generated answer:\n",
      "The arithmetic mean is the average of a set of numbers. To calculate it, you add up all the numbers in the set and divide the sum by the number of numbers in the set. For example, if you have the numbers 1, 2, 3, 4, and 5, the\n",
      "\n",
      "21\n",
      "Reference answer:\n",
      "One method for detecting spam emails is to use a machine learning model that is trained on examples of known spam emails. The model can then analyze new emails for patterns that are indicative of spam emails, such as the presence of certain keywords. Additionally, filters can be used to block emails from suspicious senders and detect emails with suspicious content.\n",
      "\n",
      "Generated answer:\n",
      "A method for detecting spam emails involves using a combination of techniques such as content analysis, sender reputation, and machine learning algorithms. Content analysis involves analyzing the content of the email to identify keywords and phrases that are commonly associated with spam. Sender reputation involves checking the reputation of the sender to determine\n",
      "\n",
      "22\n",
      "Reference answer:\n",
      "The program should take two integers as input and return their sum as output. It should be written in a programming language such as Python, Java, or Javascript.\n",
      "\n",
      "// Sample code:\n",
      "\n",
      "// Start of program\n",
      "int a;\n",
      "int b;\n",
      "int sum;\n",
      "\n",
      "a = input(); // read first number from the user\n",
      "b = input(); // read second number from the user\n",
      "sum = a + b; // calculate sum\n",
      "print(sum); // print the output\n",
      "// End of program\n",
      "\n",
      "Generated answer:\n",
      "def add_two_numbers(a, b):\n",
      "    return a + b\n",
      "\n",
      "print(add_two_numbers(2, 3)) # Output: 5\n",
      "print(add_two_numbers(10, 20)) # Output: 30\n",
      "print(\n",
      "\n",
      "23\n",
      "Reference answer:\n",
      "To reduce the number of distractions when studying, it is best to have a dedicated workspace that is free from clutter and other distractions such as noise and technology. It is also important to set a schedule and stick to it, as well as take regular breaks. Additionally, it is beneficial to practice meditation or deep breathing to stay focused and clear the mind.\n",
      "\n",
      "Generated answer:\n",
      "One way to reduce distractions when studying is to create a dedicated study space. This space should be free from any distractions such as television, music, or other people. Additionally, it is important to set a timer and stick to a schedule. This will help to keep you focused and on task\n",
      "\n",
      "24\n",
      "Reference answer:\n",
      "Algorithm:\n",
      "1. Initialize Sum as 0.\n",
      "2. Set a variable i to 0.\n",
      "3. While i is less than or equal to 100, do the following: \n",
      "  a. If i is divisible by 5 or 3, add the value of i to Sum.\n",
      "  b. Increment i by 1.\n",
      "4. Return the value of Sum.\n",
      "\n",
      "Generated answer:\n",
      "Algorithm:\n",
      "1. Initialize Sum as 0.\n",
      "2. Set a variable i to 0.\n",
      "3. While i is less than or equal to 100, do the following: \n",
      "  a. If i is divisible by 5 or 3, add the value of\n",
      "\n",
      "25\n",
      "Reference answer:\n",
      "Government\n",
      "\n",
      "Generated answer:\n",
      "Government\n",
      "\n",
      "\n",
      "26\n",
      "Reference answer:\n",
      "• What led you to pursue a career in software development?\n",
      "• Describe your experience with developing software applications.\n",
      "• What challenges have you faced in software development and how did you overcome them?\n",
      "• What techniques do you use to debug code?\n",
      "• How do you keep yourself up-to-date with the latest software development technologies and trends?\n",
      "• What type of software development project are you most proud of and why?\n",
      "• How do you ensure code quality and performance?\n",
      "• Describe your experience with Agile/Scrum methodology.\n",
      "• Talk about a time when you had to take initiative to solve a problem during a software development project.\n",
      "• Describe an instance when you had to collaborate with a team member to develop a complex software feature.\n",
      "• How do you handle difficult stakeholders?\n",
      "• Describe a difficult decision you made while working on a software development project.\n",
      "• Talk about a time when you had to adjust to a change in requirements and how you handled it.\n",
      "\n",
      "Generated answer:\n",
      "1. What experience do you have in software development?\n",
      "2. What programming languages are you proficient in?\n",
      "3. How do you approach problem-solving?\n",
      "4. What is your experience with agile development?\n",
      "5. How do you stay up to date with the latest\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def human_evaluation_print(dataset, generated_responses):\n",
    "\n",
    "    # Make sure you have the correct number of responses\n",
    "    assert len(dataset) == len(generated_responses), \"The number of generated responses must match the number of dataset entries\"\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        reference_answer = dataset[i]['output']\n",
    "        generated_answer = generated_responses[i]\n",
    "        print(i)\n",
    "        print(f\"Reference answer:\\n{reference_answer}\\n\\nGenerated answer:\\n{generated_answer}\\n\")\n",
    "\n",
    "human_evaluation_print(test_dataset, generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scores for Each Sample:**\n",
    "\n",
    "1. **Sample 1 (Arthritis Exercise):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 0.8 (The generated answer mentions cycling, which is not typically considered a low-impact exercise for arthritis.)\n",
    "   - **Average: 0.93**\n",
    "\n",
    "2. **Sample 2 (Atomic Mass of Lithium):**\n",
    "   - Grammatical Correctness: 0.8 (The generated answer has a repetitive sentence.)\n",
    "   - Coherence: 0.8 (The generated answer is not coherent due to the repetition.)\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 0.87**\n",
    "\n",
    "3. **Sample 3 (ASCII Characters):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 0 (The generated answer is incorrect.)\n",
    "   - **Average: 0.67**\n",
    "\n",
    "4. **Sample 4 (Fear Description):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 1**\n",
    "\n",
    "5. **Sample 5 (Skateboard Promotion):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 0.9 (The generated answer adds an incomplete sentence about social media.)\n",
    "   - **Average: 0.97**\n",
    "\n",
    "6. **Sample 6 (Picnic Vacation):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 0.8 (The generated answer is cut off, making it less coherent.)\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 0.93**\n",
    "\n",
    "7. **Sample 7 (Big Data):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 0.8 (The generated answer is incomplete and less coherent.)\n",
    "   - Correctness of Answer: 0.8 (The generated answer misses the aspect of customer interaction.)\n",
    "   - **Average: 0.87**\n",
    "\n",
    "8. **Sample 8 (Wild Goose Chase):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 1**\n",
    "\n",
    "9. **Sample 9 (Journals):**\n",
    "   - Grammatical Correctness: 1\n",
    "   - Coherence: 1\n",
    "   - Correctness of Answer: 1\n",
    "   - **Average: 1**\n",
    "\n",
    "10. **Sample 10 (Countries):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "11. **Sample 11 (Sailboat):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer adds unnecessary details.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "12. **Sample 12 (Humor Importance):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "13. **Sample 13 (Hypotenuse Length):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0 (The generated answer provides an incorrect value.)\n",
    "    - **Average: 0.67**\n",
    "\n",
    "14. **Sample 14 (Symphony Orchestra):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer adds unnecessary details.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "15. **Sample 15 (GPT Description):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "16. **Sample 16 (Paris):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer adds unnecessary details.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "17. **Sample 17 (Understanding Customer's Needs):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer is less specific and comprehensive.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "18. **Sample 18 (Cooking Pork Chops):**\n",
    "    - Grammatically Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "19. **Sample 19 (Customer Service):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 0.8 (The generated answer is a bit disjointed.)\n",
    "    - Correctness of Answer: 0.8 (The generated answer doesn't address the original query about order arrival.)\n",
    "    - **Average: 0.87**\n",
    "\n",
    "20. **Sample 20 (Arithmetic Mean):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "21. **Sample 21 (Detecting Spam Emails):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "22. **Sample 22 (Sum Program):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "23. **Sample 23 (Reducing Distractions):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "24. **Sample 24 (Sum Algorithm):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "25. **Sample 25 (Government):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 1\n",
    "    - **Average: 1**\n",
    "\n",
    "26. **Sample 26 (Software Development Questions):**\n",
    "    - Grammatical Correctness: 1\n",
    "    - Coherence: 1\n",
    "    - Correctness of Answer: 0.8 (The generated answer is less comprehensive.)\n",
    "    - **Average: 0.93**\n",
    "\n",
    "**Overall Average Score:** 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.38705383834704016\n",
      "Rouge-L: 0.5804612348410141\n",
      "BERTScore: 0.9290164444181654\n",
      "Perplexity: 21.452082633972168\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def evaluate_model_performance(dataset, generated_responses):\n",
    "    # Initialize metrics and lists to save answers\n",
    "    bleu_scores = []\n",
    "    rouge_l_scores = []\n",
    "    bert_f1_scores = []\n",
    "    perplexity_scores = []\n",
    "\n",
    "    # Make sure you have the correct number of responses\n",
    "    assert len(dataset) == len(generated_responses), \"The number of generated responses must match the number of dataset entries\"\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        reference_answer = dataset[i]['output']\n",
    "        generated_answer = generated_responses[i]\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        bleu_score = corpus_bleu([generated_answer], [[reference_answer]])\n",
    "        bleu_score_normalized = bleu_score.score / 100.0\n",
    "        bleu_scores.append(bleu_score_normalized)\n",
    "        \n",
    "        rouge_l_scores.append(rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True).score(reference_answer, generated_answer)['rougeL'].fmeasure)\n",
    "        \n",
    "        bert_f1_scores.append(score([generated_answer], [reference_answer], lang='en')[2].mean().item())\n",
    "\n",
    "        # Calculate perplexity\n",
    "        # encodings = tokenizer(generated_answer, return_tensors='pt')\n",
    "        # with torch.no_grad():\n",
    "        #     outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "        #     loss = outputs.loss\n",
    "        #     perplexity = torch.exp(loss).item()\n",
    "        # perplexity_scores.append(perplexity)\n",
    "        # Check if generated_answer is not empty\n",
    "        if len(generated_answer) > 0:\n",
    "            encodings = tokenizer(generated_answer, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "                loss = outputs.loss\n",
    "                perplexity = torch.exp(loss).item()\n",
    "            perplexity_scores.append(perplexity)\n",
    "        else:\n",
    "            # Handle empty generated_answer, e.g., by appending a default value or skipping\n",
    "            perplexity_scores.append(0.0)\n",
    "\n",
    "\n",
    "    # Calculate average scores\n",
    "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "    average_bert_f1 = sum(bert_f1_scores) / len(bert_f1_scores)\n",
    "    average_perplexity = sum(perplexity_scores) / len(perplexity_scores)\n",
    "\n",
    "    # Print results\n",
    "    print(f'BLEU: {average_bleu}')\n",
    "    print(f'Rouge-L: {average_rouge_l}')\n",
    "    print(f'BERTScore: {average_bert_f1}')\n",
    "    print(f'Perplexity: {average_perplexity}')\n",
    "\n",
    "    return average_bleu, average_rouge_l, average_bert_f1, average_perplexity\n",
    "\n",
    "average_bleu, average_rouge_l, average_bert_f1, average_perplexity = evaluate_model_performance(test_dataset, generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=10, fixed_beam_size=1, fixed_temperature=0.8:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=10, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:03<00:00, 31.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.3510541020017856\n",
      "Rouge-L: 0.5341504758440547\n",
      "BERTScore: 0.9189764857292175\n",
      "Perplexity: 22.09542437835976\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=25, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:05<00:00, 32.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.33892633976197184\n",
      "Rouge-L: 0.5354307816020911\n",
      "BERTScore: 0.9207236060389766\n",
      "Perplexity: 24.39620429498178\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=40, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:04<00:00, 32.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.3477686370804529\n",
      "Rouge-L: 0.5360981619219823\n",
      "BERTScore: 0.9177809666704249\n",
      "Perplexity: 22.665053650184912\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for top_k=75, fixed_beam_size=1, fixed_temperature=0.8: 100%|██████████| 2/2 [01:08<00:00, 34.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.35338924735824695\n",
      "Rouge-L: 0.5443096117295361\n",
      "BERTScore: 0.923617321032065\n",
      "Perplexity: 19.356942353425204\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "top_k_values = [10, 25, 40, 75]\n",
    "beam_sizes = [2, 4, 6, 8]\n",
    "temperatures = [0.25, 0.5, 0.7, 1.0]\n",
    "\n",
    "# Varying top_k while keeping beam_size and temperature fixed\n",
    "fixed_beam_size = 1\n",
    "fixed_temperature = 0.8\n",
    "\n",
    "for top_k in top_k_values:\n",
    "    key = f\"top_k={top_k}, fixed_beam_size={fixed_beam_size}, fixed_temperature={fixed_temperature}\"\n",
    "    generated_output_2 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_2 = pipe(prompts, max_new_tokens=64, top_k=top_k, num_beams=fixed_beam_size, temperature=fixed_temperature, do_sample=True)\n",
    "\n",
    "        for result in results_2:\n",
    "            generated_text_2 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_2.append(generated_text_2)\n",
    "    \n",
    "    # generated_texts_2 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_2 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_2]\n",
    "    \n",
    "    average_bleu_2, average_rouge_l_2, average_bert_f1_2, average_perplexity_2 = evaluate_model_performance(test_dataset, generated_texts_2)\n",
    "\n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=2, fixed_temperature=0.8:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=2, fixed_temperature=0.8: 100%|██████████| 2/2 [01:07<00:00, 33.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.38100607461065433\n",
      "Rouge-L: 0.5782171854689289\n",
      "BERTScore: 0.9302632764533714\n",
      "Perplexity: 18.543787055545383\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=4, fixed_temperature=0.8: 100%|██████████| 2/2 [01:11<00:00, 35.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.4246186741839497\n",
      "Rouge-L: 0.5945756920947856\n",
      "BERTScore: 0.9354002210828993\n",
      "Perplexity: 21.820143734967267\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=6, fixed_temperature=0.8: 100%|██████████| 2/2 [01:12<00:00, 36.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.3855511581025339\n",
      "Rouge-L: 0.5651251978481594\n",
      "BERTScore: 0.9277476734585233\n",
      "Perplexity: 20.6197026570638\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, beam_size=8, fixed_temperature=0.8: 100%|██████████| 2/2 [01:17<00:00, 38.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.42297149285432706\n",
      "Rouge-L: 0.5941208357438542\n",
      "BERTScore: 0.937205934966052\n",
      "Perplexity: 21.74965833734583\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "fixed_top_k = 50\n",
    "\n",
    "for beam_size in beam_sizes:\n",
    "    key = f\"fixed_top_k={fixed_top_k}, beam_size={beam_size}, fixed_temperature={fixed_temperature}\"\n",
    "    generated_output_3 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_3 = pipe(prompts, max_new_tokens=64, top_k=fixed_top_k, num_beams=beam_size, temperature=fixed_temperature, do_sample=True)\n",
    "\n",
    "        for result in results_3:\n",
    "            generated_text_3 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_3.append(generated_text_3)\n",
    "    \n",
    "    # generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_3]\n",
    "\n",
    "    average_bleu_3, average_rouge_l_3, average_bert_f1_3, average_perplexity_3 = evaluate_model_performance(test_dataset, generated_texts_3)\n",
    "    \n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.25:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.25: 100%|██████████| 2/2 [01:01<00:00, 30.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.41189804190591417\n",
      "Rouge-L: 0.595131729722499\n",
      "BERTScore: 0.9346063159130238\n",
      "Perplexity: 21.931470959274858\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.5: 100%|██████████| 2/2 [01:03<00:00, 31.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.37135443452174927\n",
      "Rouge-L: 0.5510741890311018\n",
      "BERTScore: 0.9246968582824424\n",
      "Perplexity: 42.8096085301152\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=0.7: 100%|██████████| 2/2 [01:05<00:00, 32.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.31217096451696497\n",
      "Rouge-L: 0.5130026279064706\n",
      "BERTScore: 0.9176002939542135\n",
      "Perplexity: 19.12071810828315\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text for fixed_top_k=50, fixed_beam_size=1, temperature=1.0: 100%|██████████| 2/2 [01:03<00:00, 31.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.32099546072633484\n",
      "Rouge-L: 0.5092024002859925\n",
      "BERTScore: 0.9161249796549479\n",
      "Perplexity: 24.89751501436587\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for temperature in temperatures:\n",
    "    key = f\"fixed_top_k={fixed_top_k}, fixed_beam_size={fixed_beam_size}, temperature={temperature}\"\n",
    "    generated_output_4 = []  # Reset the generated_output_2 for each top_k value\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=f\"Generating text for {key}\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'].split('\\n\\n### Response:\\n')[0] for example in batch]\n",
    "        results_4 = pipe(prompts, max_new_tokens=64, top_k=fixed_top_k, num_beams=fixed_beam_size, temperature=temperature, do_sample=True)\n",
    "\n",
    "        for result in results_4:\n",
    "            generated_text_4 = result[0]['generated_text']  # Access the first element of the inner list\n",
    "            generated_output_4.append(generated_text_4)\n",
    "    \n",
    "    # generated_texts_3 = [x.split('\\n\\n### Response:\\n')[1] for x in generated_output]\n",
    "    generated_texts_4 = [x.split('\\n\\n### Response:\\n')[1] if '\\n\\n### Response:\\n' in x else \"\" for x in generated_output_4]\n",
    "\n",
    "    average_bleu_4, average_rouge_l_4, average_bert_f1_4, average_perplexity_4 = evaluate_model_performance(test_dataset, generated_texts_4)\n",
    "    \n",
    "    # print(f\"Results for top_k={top_k}, beam_size={fixed_beam_size}, temperature={fixed_temperature}: BLEU={average_bleu_2}, Rouge-L={average_rouge_l_2}, BERTScore={average_bert_f1_2}, Perplexity={average_perplexity_2}\")\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
